{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Generate reports from medical tags.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVvZWvHQI_Is"
      },
      "source": [
        "# Generate sections of Radiology reports from a given table contains medical tags\n",
        "\n",
        "1. The input tabels for this notebook can be found in the \"Data\" folder in the main Git page\n",
        "2. The input and output paths should be changes to your desired paths in order to get the results "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHZ5W-ocJexj"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2FjDEiDI_gr"
      },
      "source": [
        "!pip install transformers -q\n",
        "!pip install wandb -q\n",
        "!pip install sentencepiece -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRP-AR9_bdBA"
      },
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# WandB – Import the wandb library\n",
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl2_UdDfbgDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "287c430d-631d-4ef8-8750-1e774e09685d"
      },
      "source": [
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'This notebook is running with device={device}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This notebook is running with device=cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lbhlqSuPQGz",
        "outputId": "fae9815c-9e64-4182-b8d8-4195d55134f1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8dtyyTCbz7l",
        "outputId": "bfc58ade-4acd-4385-af35-76e4bd90064b"
      },
      "source": [
        "# Login to wandb to log the model run and all the parameters\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXjQrnuPJm6A"
      },
      "source": [
        "## Methods "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6Ej4vL7rnOh"
      },
      "source": [
        "def save_model(model,EPOCH,file_name,LOSS):\n",
        "    #save the model into desired location pc/drive\n",
        "\n",
        "    #Parameters for saving:\n",
        "    #    model :the model \n",
        "    #    epoch :the current epoch\n",
        "    #    loss :the current loss\n",
        "    #    file_name :the name of the output for saving\n",
        "\n",
        "  #PATH = '/content/drive/My Drive/Colab Notebooks/Final project - Zebra/saved models/' +file_name+'E'+str(EPOCH)+'.pt' #change the path to your folders\n",
        "  torch.save({\n",
        "              'epoch': EPOCH,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss': LOSS,\n",
        "              }, PATH)\n",
        "\n",
        "\n",
        "def load_model(PATH, model, optimizer, train=True):\n",
        "    #load the model from pc/drive\n",
        "\n",
        "    #Parameters for saving:\n",
        "    #    path :the path to load from the file\n",
        "    #    model :model which the data from the file will be loaded into\n",
        "    #    optimizer :optimizer which the data from the file will be loaded into\n",
        "    #    train :boolean status for decide if train or eval\n",
        "\n",
        "  checkpoint = torch.load(PATH)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  epoch = checkpoint['epoch']\n",
        "  loss = checkpoint['loss']\n",
        "\n",
        "  if train:\n",
        "    model.train()\n",
        "  else:\n",
        "    model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MajdMA79-nmQ"
      },
      "source": [
        "\n",
        "class CustomDataset(Dataset):\n",
        "    # Creating a custom dataset for reading the dataframe and loading it into the dataloader\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.text = self.data.text\n",
        "        self.ctext = self.data.ctext\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ctext = str(self.ctext[index])\n",
        "        ctext = ' '.join(ctext.split())\n",
        "\n",
        "        text = str(self.text[index])\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_mask' : target_mask.to(dtype=torch.long)\n",
        "            #'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8WjtR-ccUBs"
      },
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    #train the given model on a given dataloder\n",
        "\n",
        "    #Parameters:\n",
        "    #    epoch :number of epochs\n",
        "    #    tokenizer :tokenzier for encoding the text\n",
        "    #    model :the model to train\n",
        "    #    device :CPU or GPU\n",
        "    #    loader :the data for training\n",
        "    #    optimizer :optimizer for training\n",
        "\n",
        "    #Returns:\n",
        "    #    print the loss each 500 batches\n",
        "\n",
        "    model.train()\n",
        "    for _,data in enumerate(loader, 0):\n",
        "        \n",
        "        lm_labels = data['target_ids'].to(device, dtype = torch.long)\n",
        "        lm_labels[lm_labels == tokenizer.pad_token_id] = -100\n",
        "        decoder_attention_mask = data['target_mask'].to(device, dtype = torch.long)\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_attention_mask = decoder_attention_mask, labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "        \n",
        "        if _%10 == 0:\n",
        "            wandb.log({\"Training Loss\": loss.item()})\n",
        "\n",
        "        if _%500==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_2YJbrzcZVT"
      },
      "source": [
        "def validate(tokenizer, model, device, loader):\n",
        "    #evaluate the given model on a given data\n",
        "\n",
        "    #Parameters:\n",
        "    #    tokenizer :tokenzier for encoding the text\n",
        "    #    model :the model to train\n",
        "    #    device :CPU or GPU\n",
        "    #    loader :the data for evaluating\n",
        "\n",
        "    #Returns:\n",
        "    #    predictions - the generated text\n",
        "    #    actuals - the ground truth\n",
        "    #    inputs - the input text\n",
        "    #    BLEUs - the blue score between predictions and actuals for evaluation\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    inputs = []\n",
        "    BLEUs = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask, \n",
        "                max_length=175, \n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True,\n",
        "                #num_return_sequences=3\n",
        "                #min_length = 50,\n",
        "                )\n",
        "                \n",
        "            #print(generated_ids.shape)\n",
        "            if data['target_ids'].shape[0]==2:\n",
        "              preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "              target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "              input = [tokenizer.decode(p, skip_special_tokens=True, clean_up_tokenization_spaces=True)for p in ids]\n",
        "              BLEU = [sentence_bleu([target[0].split(' ')],preds[0].split(' ')), sentence_bleu([target[1].split(' ')],preds[1].split(' '))]\n",
        "              if _%100==0:\n",
        "                  print(f'Completed {_}')\n",
        "             \n",
        "              predictions.extend(preds)\n",
        "              actuals.extend(target)\n",
        "              inputs.extend(input)\n",
        "              BLEUs.extend(BLEU)\n",
        "    \n",
        "\n",
        "    return predictions, actuals, inputs , BLEUs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMADe8aHJrPr"
      },
      "source": [
        "## Main "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SuLpo77cf2K"
      },
      "source": [
        "# WandB – Initialize a new run\n",
        "wandb.init(project=\"transformers_report_generation_new\", entity='zebra-idc')\n",
        "\n",
        "# Hyperparameters and inputs\n",
        "config = wandb.config          # Initialize config\n",
        "config.TRAIN_BATCH_SIZE = 4    # input batch size for training \n",
        "config.VALID_BATCH_SIZE = 2    # input batch size for testing \n",
        "config.TRAIN_EPOCHS = 1        # number of epochs to train \n",
        "config.LEARNING_RATE = 1e-4    # learning rate \n",
        "config.SEED = 42               # random seed (default: 42)\n",
        "config.MAX_LEN = 512\n",
        "config.SUMMARY_LEN = 150 \n",
        "\n",
        "# Set random seeds and deterministic pytorch for reproducibility\n",
        "torch.manual_seed(config.SEED) # pytorch random seed\n",
        "np.random.seed(config.SEED) # numpy random seed\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# tokenzier for encoding the text\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "# Import the input table for the model\n",
        "# Need to change it for each input we add to the model \n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Final project - Zebra/Data/chexbert&pert reports - train.csv',encoding='latin-1')\n",
        "\n",
        "#deleting rows with empty cells in 'report_impression' and 'report_findings' columns\n",
        "df = df[pd.notna(df['report_impression'])]\n",
        "df = df[pd.notna(df['report_findings'])]\n",
        "\n",
        "# Concatenate the medical tags with different columns from the table\n",
        "df['concatenate_input'] = df['chexbert medical tags'] + '. ' + df['report_findings']\n",
        "df = df[['concatenate_input','report_impression']] # define dataframe that contain the input on column and ground_truth on the other column \n",
        "df= df.reset_index(drop=True)\n",
        "df = df.rename(columns={'chexbert medical tags': 'ctext', 'report_impression': 'text'})\n",
        "#print(df.head())\n",
        "\n",
        "\n",
        "# Split Datasets to train and validation \n",
        "# Defining the train size. So 97% of the data will be used for training and the rest will be used for validation. \n",
        "train_size = 0.97\n",
        "train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n",
        "val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Add pre defined 30 instances (removed from the train set)\n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Final project - Zebra/Data/chexbert&pert reports - test.csv',encoding='latin-1') #change the output path to yours\n",
        "\n",
        "#deleting rows with empty cells in 'report_impression' and 'report_findings' columns\n",
        "df = df[pd.notna(df['report_impression'])]\n",
        "df = df[pd.notna(df['report_findings'])]\n",
        "\n",
        "\n",
        "# Concatenate the medical tags with different columns from the table\n",
        "df['concatenate_input'] =df['chexbert medical tags'] + '. ' + df['report_findings']\n",
        "df = df[['concatenate_input','report_impression']] # define dataframe that contain the input on column and ground_truth on the other column \n",
        "df= df.reset_index(drop=True)\n",
        "df = df.rename(columns={'chexbert medical tags': 'ctext', 'report_impression': 'text'})\n",
        "# Concat the pre defined 30 instances with the validation set\n",
        "val_dataset = pd.concat([df, val_dataset], ignore_index=True) \n",
        "\n",
        "#print shapes\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
        "\n",
        "# Creating the Training and Validation dataset for further creation of Dataloader\n",
        "training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "\n",
        "# Defining the parameters for creation of the dataloaders\n",
        "train_params = {\n",
        "    'batch_size': config.TRAIN_BATCH_SIZE,\n",
        "    'shuffle': True,\n",
        "    'num_workers': 0\n",
        "    }\n",
        "\n",
        "val_params = {\n",
        "    'batch_size': config.VALID_BATCH_SIZE,\n",
        "    'shuffle': False,\n",
        "    'num_workers': 0\n",
        "    }\n",
        "\n",
        "# Creation of Dataloaders for training and validation. \n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\",output_attentions=True)\n",
        "model = model.to(device)\n",
        "\n",
        "# Defining the optimizer \n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "# Log metrics with wandb\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "# Training loop\n",
        "print('--------Training---------')\n",
        "for epoch in range(config.TRAIN_EPOCHS):\n",
        "    train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "\n",
        "# Validation loop\n",
        "print('--------Validation---------')\n",
        "predictions, actuals, inputs, BLEUs = validate(tokenizer, model, device, val_loader)\n",
        "final_df = pd.DataFrame({'Input Text':inputs, 'Actual Text':actuals, 'Generated Text':predictions, 'BLEU Validation':BLEUs}) #create df from results\n",
        "final_df.to_csv('/content/drive/My Drive/Colab Notebooks/Final project - Zebra/prediction_styling_history_addedInformation.csv') #change the output path to yours\n",
        "print('Output Files generated for review')\n",
        "\n",
        "#save_model(model,config.TRAIN_EPOCHS,'findings chexpert_',0.1) # Save the model \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}